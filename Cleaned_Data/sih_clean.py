# -*- coding: utf-8 -*-
"""SIH_CLEAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AkfzQKwb1mANaGYlRFLWMLxfSfnrzFGS
"""

import glob
import nltk
nltk.download('stopwords')
import pandas as pd
import io
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import string
from nltk.stem import WordNetLemmatizer
 
lemmatizer = WordNetLemmatizer()



stop_words = set(stopwords.words('english')) 

path = 'D:\\SIH\\Prashna\\Cleaned_Data\\Data_Chem'
filelist = glob.glob(path + "\\*.csv")
for file in filelist:
    print(file)  
    df = pd.read_csv(file, sep=';')
    
    pf=df.values.tolist()

    dff=[]
    for line in pf:
      dff=dff+str(line).split()


    
    final=pd.DataFrame(data=dff, index=None, columns=['A'])
    


    # filteredtext1 = []
    # for word in final['A']:
      
    #   if word.lower() not in stop_words:
    #     filteredtext1.append(word)
    # filteredtext1

    # cleanfiltered=[]
    # for txt in filteredtext1:
    #   txt = txt.replace(' ','')
          
    #   x = re.search("\([a-zA-Z]\)", txt)
    #   if(x!=None):
    #     txt=txt[:x.start()]+txt[x.end():]
    #   if txt.lower() not in stop_words:
    #     if(len(txt)>1):
    #       txt = txt.replace('(','')
    #       txt = txt.replace(')','')
    #       txt = txt.replace('-','')
    #       txt = txt.replace('_','')
          
    #       cleanfiltered.append(txt.lower().strip())
    
    #CLEANING DATA
    stop_words = set(stopwords.words('english')) 
    cleanfiltered=[]
    for txt in final['A']:
  
      x = re.search("\([\w]\)", txt)
      if(x!=None):
        txt=txt[:x.start()]+txt[x.end():]
      x = re.search("\([\w]\)", txt)
      if(x!=None):
        txt=txt[:x.start()]+txt[x.end():]


      if(txt.startswith('\\')or(txt.startswith('[')or(txt.startswith(string.punctuation)))):
         txt=""   
  
      txt=re.sub(r"[^a-zA-Z]","",txt)
      if txt.lower() not in stop_words:
        if(txt!=""):
          txt=lemmatizer.lemmatize(txt)      
          cleanfiltered.append(txt.lower().strip())

       
#REMOVING DUPLICATES
   
    cleanfiltered[:]=list(set(cleanfiltered))
    print(len(cleanfiltered))
    
#SAVING DATA TO CSV
    tr=pd.DataFrame(data=cleanfiltered, index=None, columns=['Words'])
    tr=tr[(tr.Words.astype(str).str.len()>2)]

    df = pd.concat([df.reset_index(), tr], axis=1)
    #print(df.Words)
    df.to_csv(file.rstrip(".csv")+"_cleaned"+".csv")